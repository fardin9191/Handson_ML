{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overall description:\n\n* *Visualization :* Observing dataset type,shape, data distribution type, correlation, GHI value over time.\n* *Preprocessing :* I used stadanrdscaling for removing the mean and scaling the data to unit variance. i split the dataset manually for lstm_input. For other model, I used sklearn train_test_split.\n* *Model training*: I trained three different NN model.SNN,FNN and LSTM. All hyperparameters were tuned by RANDPOMIZEDSEARCHCV. I haven't included that hyperparameter tunning part in this notebook. \n* *checking-error*: RMSE score\n* *Ensembling :* Stacking_ensemble on pretrained model.\n* *Weighted avg on prediction*: Gave weight to stacked_model,snn_model,lstm_model submission w.r.t to their accuracy on validation set.\n\n* ***My personal insight on increasing accuracy:*** \n> I should have used a fixed validation set for checking unbiased validation error of those model and regularize stacked model based on that unbiased error, instead i trained 3 model on different notebook,based on different train_validation set.I load those pretrained model,in stacked model and got biased result.And when i find this mistake, the comp was nearly at end,had no time for fixing that.This mistake resulted slightly overfit on test set. Public score:112.67 ,private score: 113.65\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"sZUqyMOixKPv","outputId":"3d4cb9c5-3113-42ab-cf5f-c9a451b5364c","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"id":"36XpLmKMxKPy","trusted":true},"cell_type":"code","source":"#file=\"/content/gdrive/My Drive/Colab Notebooks/train.csv\"\ntrain=pd.read_csv(\"../input/ieee-pes-bdc-datathon-year-2020/train.csv\")\ntest=pd.read_csv(\"../input/ieee-pes-bdc-datathon-year-2020/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"thxa0YXIxKP0","outputId":"ce83c18a-953f-4930-cc57-0c9bae3375c2","trusted":true},"cell_type":"code","source":"train.drop([\"ID\"], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"kL0UOC48xKP-","scrolled":true,"trusted":true},"cell_type":"code","source":"train.hist(bins=50,figsize=(20,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normal distribution in air_temp,pressure.Poisson distribution in wind_speed. We will apply transformation to this later."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null value :) We hate null value >_<"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15,8))\nsns.distplot(train['global_horizontal_irradiance']) #for checking distribution of GHI\nplt.xlim([-10,1602])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the GHI value is distributed in zero.Lets see correlation between features for more insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),annot=True,cmap='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix=train.corr()\nprint(corr_matrix['global_horizontal_irradiance'].sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see ,wind_direction and precipitation is less correlated with GHI. But we cant drop those column bcz it has impact on solar prediction, And dropping columns hampers model accuracy also."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_small=train[:200].copy()\nplt.style.use('ggplot')\ny=train_small['global_horizontal_irradiance']\nx=range(200)\n\nplt.plot(x,y,color='k')\nplt.xlabel(\"Index\")\nplt.ylabel(\"GHI\")\n#Give graph a title\nplt.title('GHI visualization')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a time series data, We can use RNN lstm on this data."},{"metadata":{},"cell_type":"markdown","source":"From above visualization and training on various model, I decided to train a \n1.SNN model(Selu+lecun_normal),as there is a great chance for exploding gradient\n 2.A FNN model(He+he_normal and \n 3.LSTM model for time series.\n\n**For LSTM, I didnt shuffle the dataset after preprocessing."},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing & splitting the dataset"},{"metadata":{},"cell_type":"markdown","source":"**I used \n1.log transformation\n2.Square root transformation\n3.Boxcox transformation\n4.Standardscaling on train_data. Among them, i find standardscaling does better then all of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label=train[\"global_horizontal_irradiance\"].copy()\ntrain.drop(['global_horizontal_irradiance'], axis=1, inplace=True)\nprint(train.shape,train_label.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For lstm,(bcz we shouldnt feed shuffled data to lstm)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def processing(train_data):\n    from sklearn.preprocessing import StandardScaler\n    scaled_features = StandardScaler().fit_transform(train_data.values)\n    scaled_data=pd.DataFrame(scaled_features,index=train_data.index,columns=train_data.columns)\n\n    return scaled_data\n\nX_train_lstm=processing(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_len = len(X_train_lstm)\npct = 0.98 \ntrain_len = int(pct*data_len)\ntrain_data_lstm = X_train_lstm[:train_len]\ntrain_label_lstm = train_label[:train_len].values\n\nX_valid_lstm = X_train_lstm[train_len:]\ny_valid_lstm = train_label[train_len:].values\n\ntrain_data_lstm = train_data_lstm.values.reshape((train_data_lstm.shape[0],1,train_data_lstm.shape[1])) #for the lstm shape (None,time step,features)\nX_valid_lstm = X_valid_lstm.values.reshape((X_valid_lstm.shape[0], 1, X_valid_lstm.shape[1]))\nprint(train_data_lstm.shape, train_label_lstm.shape, X_valid_lstm.shape,y_valid_lstm.shape) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For SNN and FNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=processing(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, X_valid, train_label, y_valid = train_test_split(train_data, train_label, train_size=0.95,test_size = 0.05, random_state = 42)\nprint(train_data.shape,train_label.shape,X_valid.shape,y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***MODEL TRAINING***\n***ALL HYPARPARAMETER WERE TUNED BY RANDOMIZEDSEARCHCV***"},{"metadata":{"id":"g5ipKSxuxKQb","trusted":true},"cell_type":"code","source":"from tensorflow import keras\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.layers import AlphaDropout\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras import models\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Flatten","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*SNN model\nACTIVATION=\"SELU\",INIT=\"LECUN_NORMAL\"*"},{"metadata":{"id":"FPFhpYnFxKQn","trusted":false},"cell_type":"code","source":"#reg = keras.regularizers.l1_l2(l1=0.01, l2=0.1) \nreg = keras.regularizers.l1_l2(l1=0.001, l2=0.01) \ndropout=0.2\nsnn_model = keras.models.Sequential([\n        \n        keras.layers.Input(shape=train_data.shape[1]),\n        keras.layers.Dense(600, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.2),\n    \n        keras.layers.Dense(600, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.2),\n    \n        keras.layers.Dense(300, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n     \n        keras.layers.Dense(300, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n    \n        keras.layers.Dense(150, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n    \n        keras.layers.Dense(150, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(150, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(100, activation=\"selu\",kernel_initializer=\"lecun_normal\", kernel_regularizer=reg),\n        keras.layers.Dense(1)\n])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\noptimizer=keras.optimizers.Nadam(\n    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-09\n)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*FNN MODEL\nACTIVATION=\"ELU\",INIT=\"HE_NORMAL\" WITH BATCHNORMALIZATION*"},{"metadata":{"trusted":false},"cell_type":"code","source":"reg = keras.regularizers.l1_l2(l1=0.001, l2=0.01) \ndropout=0.2\ninitializer = tf.keras.initializers.he_normal()\nfnn_model = keras.models.Sequential([\n        \n        keras.layers.Input(shape=train_data.shape[1]),\n        keras.layers.Dense(500, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(dropout),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(500, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(dropout),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(300, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(300, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.1),\n        keras.layers.Dense(150, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.1),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(150, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(150, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        #keras.layers.AlphaDropout(0.05),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dense(100, activation=\"elu\",kernel_initializer=initializer, kernel_regularizer=reg),\n        keras.layers.Dense(1)\n])\n# def exponential_decay(lr0, s):\n#     def exponential_decay_fn(epoch):\n#         return lr0 * 0.1**(epoch / s)\n#     return exponential_decay_fn\n# exponential_decay_fn = exponential_decay(lr0=0.01, s=100)\n#lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n\noptimizer=keras.optimizers.Nadam(\n    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-06\n)\n#lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*LSTM Model"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"reg = keras.regularizers.l1_l2(l1=0.0001, l2=0.01) \n\nlstm_model = keras.models.Sequential([\n    keras.layers.Input(shape=(train_data_lstm.shape[1], train_data_lstm.shape[2])),\n    keras.layers.LSTM(500,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.Dropout(0.2),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.LSTM(300,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.1),\n    \n    keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.1),\n    \n    keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.LSTM(100,activation=\"selu\",kernel_initializer=\"lecun_normal\",kernel_regularizer=reg),\n    keras.layers.Dense(1)\n])\n\n\noptimizer=keras.optimizers.Nadam(\n    learning_rate=0.0009, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-07)\nmodel.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"snn_model.summary()\nfnn_model.summary()\nlstm_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Model Training***"},{"metadata":{},"cell_type":"markdown","source":"**SNN"},{"metadata":{"trusted":false},"cell_type":"code","source":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"snn_model_fromcallback_on_tuned_param.h5\",\n save_best_only=True)\nreduce_lr1 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0) #For stucked at plateu problem\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=3, min_lr=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"mwzNqrhCxKQt","scrolled":false,"trusted":false},"cell_type":"code","source":"history1 = snn_model.fit(train_data, train_label, epochs=130, batch_size=150, validation_data=(X_valid, y_valid), verbose=1) \nhistory2 = snn_model.fit(train_data, train_label, epochs=60, batch_size=512, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr1]) \nhistory3 = snn_model.fit(train_data, train_label, epochs=40, batch_size=700, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr2]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame(history1.history).plot(figsize=(8, 5)) #for checking learning_curve\nplt.grid(True)\nplt.gca() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FNN"},{"metadata":{"trusted":false},"cell_type":"code","source":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"fnn_he_model_fromcallback_on_tuned_param.h5\",\n save_best_only=True)\nreduce_lr1 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0)\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=3, min_lr=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"history1 = fnn_model.fit(train_data, train_label, epochs=130, batch_size=256, validation_data=(X_valid, y_valid), verbose=1) \nhistory2 = fnn_model.fit(train_data, train_label, epochs=60, batch_size=512, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr1]) \nhistory3 = fnn_model.fit(train_data, train_label, epochs=40, batch_size=1024, validation_data=(X_valid, y_valid), verbose=1,callbacks=[checkpoint_cb,reduce_lr2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the learning_curve"},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame(history1.history).plot(figsize=(8, 5)) #for checking learning_curve\nplt.grid(True)\nplt.gca() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSTM"},{"metadata":{"trusted":false},"cell_type":"code","source":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"lstm_model_fromcallback_on_tuned_param.h5\",\n save_best_only=True)\nreduce_lr1 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n                              patience=5, min_lr=0)\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                              patience=4, min_lr=0)\nreduce_lr2 = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=3, min_lr=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"history1 = lstm_model.fit(train_data_lstm, train_label_lstm, epochs=130, batch_size=256, validation_data=(X_valid_lstm, y_valid-lstm), verbose=1,callbacks=[reduce_lr1]) \nhistory2 = lstm_model.fit(train_data_lstm, train_label_lstm, epochs=60, batch_size=512, validation_data=(X_valid_lstm, y_valid-lstm), verbose=1,callbacks=[checkpoint_cb,reduce_lr1]) \nhistory3 = lstm_model.fit(train_data_lstm, train_label_lstm, epochs=40, batch_size=1024, validation_data=(X_valid_lstm, y_valid-lstm), verbose=1,callbacks=[checkpoint_cb,reduce_lr2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***RMSE checking on validation set"},{"metadata":{"id":"Ip9_Rvv4xKQv","trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\npred_snn = snn_model.predict(X_valid)\nbasic_mse=mean_squared_error(y_valid,pred_snn)\nbasic_mse_snn=np.sqrt(basic_mse)\nprint(basic_mse_snn) #117.67","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train error : 104.21, validation error: 117.67"},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_fnn = fnn_model.predict(X_valid)\nbasic_mse_fnn=np.sqrt(mean_squared_error(y_valid,pred_fnn))\nprint(basic_mse_fnn) #120.19","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train error : 107.88, validation error: 120.19"},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_lstm = lstm_model.predict(X_valid_lstm)\nbasic_mse_lstm=np.sqrt(mean_squared_error(y_valid_lstm,pred_lstm))\nprint(basic_mse_lstm) #119.01","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train error : 105.36, validation error: 119.01"},{"metadata":{},"cell_type":"markdown","source":"# ENSEMBLING MODEL"},{"metadata":{"trusted":false},"cell_type":"code","source":"all_models=list()\n\nmodel=keras.models.load_model(\"snn_model_fromcallback_on_tuned_param.h5\")\nall_models.append(model)\nmodel=keras.models.load_model(\"lstm_model_fromcallback_on_tuned_param.h5\")\nall_models.append(model)\nmodel=keras.models.load_model(\"fnn_he_model_fromcallback_on_tuned_param.h5\")\nall_models.append(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def stacked_dataset(members, inputX):\n    \n    stackX=None\n    yhat=members[0].predict(inputX, verbose=0)\n    stackX=yhat\n\n    input_X_lstm=inputX.values.reshape((inputX.shape[0], 1, inputX.shape[1]))\n    yhat=members[1].predict(input_X_lstm, verbose=0)\n    stackX = np.concatenate((stackX,yhat),axis=1)\n\n    yhat=members[2].predict(inputX, verbose=0)\n    stackX = np.concatenate((stackX,yhat),axis=1)\n\n    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]))\n    return stackX    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_model(stackedX):\n    \n    \n    reg = keras.regularizers.l1_l2(l1=0.001, l2=0.01) \n    dropout=0.2\n    #initializer = tf.keras.initializers.he_normal()\n    model = keras.models.Sequential([\n                  \n      keras.layers.Input(shape=(stackedX.shape[1], stackedX.shape[2])),\n      keras.layers.LSTM(500,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.Dropout(0.2),\n      keras.layers.BatchNormalization(),\n    \n      keras.layers.LSTM(300,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.BatchNormalization(),\n      keras.layers.Dropout(0.1),\n    \n      keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.BatchNormalization(),\n      keras.layers.Dropout(0.1),\n    \n      keras.layers.LSTM(150,activation=\"selu\",kernel_initializer=\"lecun_normal\",return_sequences=True,kernel_regularizer=reg),\n      keras.layers.BatchNormalization(),\n    \n      keras.layers.LSTM(100,activation=\"selu\",kernel_initializer=\"lecun_normal\",kernel_regularizer=reg),\n      keras.layers.Dense(1)\n\n             ])\n \n    early_stopping_cb = keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True)\n    optimizer=keras.optimizers.Nadam(\n        learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07,decay=1e-06\n    )\n    model.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def dividing_stacked(stackedX,inputy):\n    data_len = len(inputy)\n    pct = 0.90 # change it to 0.8~0.9\n    test_len = int(pct*data_len)\n    X_train = stackedX[:test_len,:]\n    y_train = inputy[:test_len]\n\n    X_valid = stackedX[test_len:,:]\n    y_valid = inputy[test_len:]\n\n    X_train=X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n    X_valid =X_valid .reshape((X_valid .shape[0], 1, X_valid .shape[1]))\n\n    print(stackedX.shape,inputy.shape,X_train.shape,y_train.shape,X_valid.shape,y_valid.shape)\n    return X_train,y_train,X_valid,y_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def processing_stack(train_data):\n    \n    scaled_features = StandardScaler().fit_transform(train_data)\n    return scaled_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def fit_stacked_model(members, inputX, inputy):\n    # create dataset using ensemble\n    stackedX = stacked_dataset(members, inputX)\n    \n    stackedX = processing_stack(stackedX)\n    #make a slice for validation info\n    stackedX,inputy,stackedX_slice,inputy_slice = dividing_stacked(stackedX,inputy)\n \n    nn_model = create_model(stackedX)\n\n    history1=nn_model.fit(stackedX, inputy, epochs=130, validation_data=(stackedX_slice, inputy_slice),batch_size=256,verbose=1,callbacks=[reduce_lr1])\n    history2=nn_model.fit(stackedX, inputy, epochs=50, validation_data=(stackedX_slice, inputy_slice),batch_size=512,verbose=1,callbacks=[checkpoint_cb,reduce_lr2])\n    history3=nn_model.fit(stackedX, inputy, epochs=40, validation_data=(stackedX_slice, inputy_slice),batch_size=800,verbose=1,callbacks=[checkpoint_cb,reduce_lr3])\n    \n    return nn_model,history1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def stacked_prediction(members, model, inputX):\n    # create dataset using ensemble\n    stackedX = stacked_dataset(members, inputX)\n    #processing\n    stackedX = processing_stack(stackedX)\n    # make a prediction\n    yhat = model.predict(stackedX)\n    return yhat","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model,history = fit_stacked_model(all_models, train_data, train_label)\n# evaluate model on test set\nyhat = stacked_prediction(all_models, model, X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stacked_mse=mean_squared_error(y_valid,yhat)  # mse on validation set\nstacked_mse=np.sqrt(stacked_mse)\nprint(stacked_mse)  #115.42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation error : 115.42"},{"metadata":{},"cell_type":"markdown","source":"# TEST set prediction"},{"metadata":{},"cell_type":"markdown","source":"We will take the outpput from three model. The stacked model, SNN model and LSTM model, And apply weighted avg on them."},{"metadata":{},"cell_type":"markdown","source":"***WEIGHTED AVERAGE On 3 PREDICTION***\n**from previous:\n*    stacked model rmse on validation set: 115.42\n*    lstm model rmse on validation set: 119.01\n*    snn model rmse on validation set: 117.67\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"test=pd.read_csv(\"test.csv\")\ntest_ID = test['ID'].values.reshape(len(test))\ntest.drop(['ID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test=processing(test)\nprint(X_test.columns,X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_lstm = X_test.values.reshape((X_test.shape[0],1,X_test.shape[1])) # preparing for lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_pred_stacked = stacked_prediction(all_models, model, X_test) #prediction of stacked model (validation set rmse:)\n\ntest_pred_lstm = lstm_model.predict(X_test_lstm) #prediction of previously trained lstm\n\ntest_pred_snn = snn_model.predict(X_test) #prediction of previously trained snn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"coefs = [0.60, 0.25, 0.15]\n\ntest_pred = test_pred_stacked * coefs[0] + test_pred_lstm * coefs[1] + test_pred_snn * coefs[2]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds = [0 if p<0 else p for p in test_pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"zippedList =  list(zip(test_ID, preds))\nsubmission = pd.DataFrame(zippedList, columns = ['ID','global_horizontal_irradiance'])\nsubmission=submission.explode('global_horizontal_irradiance')\nsubmission.to_csv('submission_level5', index=False)  #PUBLIC leaderboard score: 112.67","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}